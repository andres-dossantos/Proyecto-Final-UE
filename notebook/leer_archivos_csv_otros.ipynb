{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidación completa. Los datos consolidados se han guardado en data_total_country_xfila.csv.\n"
     ]
    }
   ],
   "source": [
    "## Este script combina los archivos CSV de data sin tener en cuenta los de \"Happines..\"\n",
    "## Los indicadores para cada año se van guardando fila por fila\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Obtener la ruta del directorio del notebook actual\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Construir la ruta relativa a la carpeta 'datos'\n",
    "folder_path = os.path.join(current_dir, '../data')\n",
    "\n",
    "# Leer el archivo JSON con la codificación correcta\n",
    "with open('data_country.json', 'r', encoding='utf-8') as file:\n",
    "    data_country = json.load(file)\n",
    "\n",
    "# Crear un diccionario inverso para los nombres alternativos\n",
    "alternate_names_to_official = {}\n",
    "for country, info in data_country.items():\n",
    "    for alt_name in info.get('AlternateNames', []):\n",
    "        alternate_names_to_official[alt_name] = country\n",
    "\n",
    "# Obtener una lista de todos los archivos CSV en la carpeta, excluyendo los que contienen \"Happiness\"\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv') and 'Happiness' not in f]\n",
    "\n",
    "# Inicializar un DataFrame vacío para almacenar todos los datos transpuestos\n",
    "all_transposed_data = pd.DataFrame()\n",
    "\n",
    "# Leer y procesar cada archivo CSV\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "    \n",
    "    try:\n",
    "        # Leer el archivo CSV con el delimitador adecuado\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        # Encontrar el índice de la fila \"Country Name\"\n",
    "        start_index = None\n",
    "        for i, line in enumerate(lines):\n",
    "            if line.startswith('\"Country Name\"'):\n",
    "                start_index = i\n",
    "                break\n",
    "        \n",
    "        if start_index is None:\n",
    "            print(f'No se encontró la fila \"Country Name\" en el archivo {csv_file}')\n",
    "            continue\n",
    "                \n",
    "        # Leer el archivo CSV desde la fila \"Country Name\"\n",
    "        data = pd.read_csv(file_path, encoding='utf-8', delimiter=',', quotechar='\"', on_bad_lines='skip', skiprows=start_index)\n",
    "        \n",
    "        # Reemplazar nombres alternativos por nombres oficiales\n",
    "        data['Country Name'] = data['Country Name'].apply(lambda x: alternate_names_to_official.get(x, x))  \n",
    "        \n",
    "        # Cambiar la información de la columna \"Indicator Name\" por el nombre de la carpeta\n",
    "        data['Indicator Name'] = os.path.basename(file_path)\n",
    "        \n",
    "        # Eliminar las columnas \"Country Code\" y \"Indicator Code\" si existen\n",
    "        if 'Country Code' in data.columns:\n",
    "            data = data.drop(columns=['Country Code'])\n",
    "        if 'Indicator Code' in data.columns:\n",
    "            data = data.drop(columns=['Indicator Code'])\n",
    "        \n",
    "        # Validar que todos los países de la primera columna se encuentren en el archivo JSON\n",
    "        valid_countries = data['Country Name'].isin(data_country.keys())\n",
    "        data_cleaned = data[valid_countries]\n",
    "        \n",
    "        # Eliminar las filas vacías del DataFrame original\n",
    "        filas_vacias = data_cleaned[data_cleaned.iloc[:, 0].isnull() | (data_cleaned.iloc[:, 0] == '')]\n",
    "        data_cleaned = data_cleaned.drop(filas_vacias.index)\n",
    "        \n",
    "        # Eliminar columnas de años que no se necesitan\n",
    "        columns_to_keep = list(data_cleaned.columns[:4]) + list(data_cleaned.columns[-10:])\n",
    "        data_cleaned = data_cleaned[columns_to_keep]\n",
    "\n",
    "        # Crear columnas adicionales para Región y Continente\n",
    "        data_cleaned['Region'] = data_cleaned['Country Name'].apply(lambda x: data_country.get(x, {}).get('Region', 'Región Desconocida'))\n",
    "        data_cleaned['Continent'] = data_cleaned['Country Name'].apply(lambda x: data_country.get(x, {}).get('Continent', 'Continente Desconocido'))\n",
    "        \n",
    "        # Crear una lista para almacenar las filas transpuestas\n",
    "        transposed_data = []\n",
    "        \n",
    "        # Se define desde que columna lee los años\n",
    "        years = data_cleaned.columns[4:]\n",
    "\n",
    "        # Iterar sobre cada fila del DataFrame original\n",
    "        for index, row in data_cleaned.iterrows():\n",
    "            country_name = row['Country Name']\n",
    "            indicator_name = row['Indicator Name']\n",
    "            region = row['Region']\n",
    "            continent = row['Continent']\n",
    "    \n",
    "            # Iterar sobre las columnas de años\n",
    "            for year in years[:-2]:  # Excluir 'Unnamed: 68' y 'Most_Recent_Value'\n",
    "                value = row[year]\n",
    "                if not pd.isna(value):\n",
    "                    transposed_data.append([country_name, indicator_name, year, value, region, continent])\n",
    "\n",
    "        # Crear un nuevo DataFrame con los datos transpuestos\n",
    "        transposed_df = pd.DataFrame(transposed_data, columns=['Country Name', 'Indicator Name', 'Year', 'Value', 'Region', 'Continent'])\n",
    "\n",
    "        # Concatenar los datos transpuestos al DataFrame principal\n",
    "        all_transposed_data = pd.concat([all_transposed_data, transposed_df], ignore_index=True)\n",
    "        \n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f'Error al procesar el archivo {csv_file}: {e}')\n",
    "\n",
    "# Ordenar el DataFrame combinado por el nombre de la columna \"Country Name\"\n",
    "all_transposed_data.sort_values(by=['Country Name', 'Indicator Name'], inplace=False)\n",
    "\n",
    "# Guardar el dataframe consolidado en un nuevo archivo CSV\n",
    "all_transposed_data.to_csv('data_total_country_xfila.csv', index=False)\n",
    "\n",
    "print(\"Consolidación completa. Los datos consolidados se han guardado en data_total_country_xfila.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidación completa. Los datos consolidados se han guardado en data_total_country_xcolumna.csv.\n"
     ]
    }
   ],
   "source": [
    "## Este script combina los archivos CSV de data sin tener en cuenta los de \"Happines..\"\n",
    "## Los indicadores estaran en las columnas junto con el año\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Obtener la ruta del directorio del notebook actual\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Construir la ruta relativa a la carpeta 'datos'\n",
    "folder_path = os.path.join(current_dir, '../data')\n",
    "\n",
    "# Leer el archivo JSON con la codificación correcta\n",
    "with open('data_country.json', 'r', encoding='utf-8') as file:\n",
    "    data_country = json.load(file)\n",
    "\n",
    "# Crear un diccionario inverso para los nombres alternativos\n",
    "alternate_names_to_official = {}\n",
    "for country, info in data_country.items():\n",
    "    for alt_name in info.get('AlternateNames', []):\n",
    "        alternate_names_to_official[alt_name] = country\n",
    "\n",
    "# Obtener una lista de todos los archivos CSV en la carpeta, excluyendo los que contienen \"Happiness\"\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv') and 'Happiness' not in f]\n",
    "\n",
    "# Inicializar un DataFrame vacío para almacenar todos los datos\n",
    "consolidated_data = pd.DataFrame()\n",
    "    \n",
    "# Leer y procesar cada archivo CSV\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "    file_name = os.path.splitext(csv_file)[0] # Devuelve el nombre del archivo sin la extensión\n",
    "        \n",
    "    try:\n",
    "        # Leer el archivo CSV con el delimitador adecuado\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        # Encontrar el índice de la fila \"Country Name\"\n",
    "        start_index = None\n",
    "        for i, line in enumerate(lines):\n",
    "            if line.startswith('\"Country Name\"'):\n",
    "                start_index = i\n",
    "                break\n",
    "        \n",
    "        if start_index is None:\n",
    "            print(f'No se encontró la fila \"Country Name\" en el archivo {csv_file}')\n",
    "            continue\n",
    "                \n",
    "        # Leer el archivo CSV desde la fila \"Country Name\"\n",
    "        data = pd.read_csv(file_path, encoding='utf-8', delimiter=',', quotechar='\"', on_bad_lines='skip', skiprows=start_index)\n",
    "        \n",
    "        # Reemplazar nombres alternativos por nombres oficiales\n",
    "        data['Country Name'] = data['Country Name'].apply(lambda x: alternate_names_to_official.get(x, x))  \n",
    "        \n",
    "        # Validar que todos los países de la primera columna se encuentren en el archivo JSON\n",
    "        valid_countries = data['Country Name'].isin(data_country.keys())\n",
    "        data_cleaned = data[valid_countries]\n",
    "        \n",
    "        # Eliminar las filas vacías del DataFrame original\n",
    "        filas_vacias = data_cleaned[data_cleaned.iloc[:, 0].isnull() | (data_cleaned.iloc[:, 0] == '')]\n",
    "        data_cleaned = data_cleaned.drop(filas_vacias.index)\n",
    "        \n",
    "        # Eliminar columnas de años que no se necesitan\n",
    "        columns_to_keep = list(data_cleaned.columns[:4]) + list(data_cleaned.columns[-10:])\n",
    "        data_cleaned = data_cleaned[columns_to_keep]\n",
    "            \n",
    "        # Cambiar la información de la columna \"Indicator Name\" por el nombre del archivo CSV más el año\n",
    "        for year in data_cleaned.columns[4:]:\n",
    "            new_column_name = f\"{file_name}_{year}\"\n",
    "            data_cleaned.rename(columns={year: new_column_name}, inplace=True)\n",
    "            data_cleaned[new_column_name] = data_cleaned[new_column_name].round(3)    \n",
    "        \n",
    "        # Crear columnas adicionales para Región y Continente si consolidated_data está vacío\n",
    "        if consolidated_data.empty:\n",
    "            data_cleaned['Region'] = data_cleaned['Country Name'].apply(lambda x: data_country.get(x, {}).get('Region', 'Región Desconocida'))\n",
    "            data_cleaned['Continent'] = data_cleaned['Country Name'].apply(lambda x: data_country.get(x, {}).get('Continent', 'Continente Desconocido'))\n",
    "        \n",
    "            # Reordenar las columnas para que 'Region' y 'Continent' estén después de 'Country Name'\n",
    "            columns_order = ['Country Name', 'Region', 'Continent'] + [col for col in data_cleaned.columns if col not in ['Country Name', 'Region', 'Continent']]\n",
    "            data_cleaned = data_cleaned[columns_order]\n",
    "        \n",
    "        # Eliminar las columnas \"Country Code\", \"Indicator Code\" y \"Indicator Name\" si existen\n",
    "        columns_to_drop = ['Country Code', 'Indicator Code', 'Indicator Name']\n",
    "        for column in columns_to_drop:\n",
    "            if column in data_cleaned.columns:\n",
    "                data_cleaned = data_cleaned.drop(columns=[column])\n",
    "\n",
    "        # Eliminar las columnas con datos vacíos\n",
    "        data_cleaned = data_cleaned.dropna(axis=1, how='all')    \n",
    "                \n",
    "        # Si es el primer archivo, inicializar el DataFrame consolidado\n",
    "        if consolidated_data.empty:\n",
    "            consolidated_data = data_cleaned\n",
    "        else:\n",
    "            # Combinar el DataFrame consolidado con el nuevo DataFrame\n",
    "            consolidated_data = pd.merge(consolidated_data, data_cleaned, on=['Country Name'], how='outer')\n",
    "            \n",
    "            # Añadir Región y Continente para los nuevos países\n",
    "            for country in data_cleaned['Country Name']:\n",
    "                if country not in consolidated_data['Country Name'].values:\n",
    "                    region = data_country.get(country, {}).get('Region', 'Región Desconocida')\n",
    "                    continent = data_country.get(country, {}).get('Continent', 'Continente Desconocido')\n",
    "                    consolidated_data.loc[consolidated_data['Country Name'] == country, 'Region'] = region\n",
    "                    consolidated_data.loc[consolidated_data['Country Name'] == country, 'Continent'] = continent\n",
    "\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f'Error al procesar el archivo {csv_file}: {e}')\n",
    "\n",
    "# Ordenar el DataFrame combinado por el nombre de la columna \"Country Name\"\n",
    "consolidated_data.sort_values(by=['Country Name'], inplace=False)\n",
    "\n",
    "# Guardar el dataframe consolidado en un nuevo archivo CSV\n",
    "consolidated_data.to_csv('data_total_country_xcolumna.csv', index=False)\n",
    "\n",
    "print(\"Consolidación completa. Los datos consolidados se han guardado en data_total_country_xcolumna.csv.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
